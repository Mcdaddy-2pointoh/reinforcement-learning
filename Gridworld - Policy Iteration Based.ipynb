{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b6690d",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04d6fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4d6eb",
   "metadata": {},
   "source": [
    "# 1. Environnment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "646618f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_conf = {\n",
    "    \"GRID_SIZE\" : 4,\n",
    "    \"GAMMA\" : 0.9,\n",
    "    \"GOAL_STATE\" : 3,\n",
    "    \"THRESHOLD\" : 0.00001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb838d9",
   "metadata": {},
   "source": [
    "# 2. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9179304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all configs into variables\n",
    "GRID_SIZE, GAMMA, GOAL_STATE, THRESHOLD = env_conf['GRID_SIZE'],  env_conf['GAMMA'], env_conf['GOAL_STATE'], env_conf['THRESHOLD']\n",
    "NUM_STATES = GRID_SIZE * GRID_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94626d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an reward map with the specifications\n",
    "rewards = np.zeros(NUM_STATES)\n",
    "rewards[GOAL_STATE] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed0444e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an action map, i.e. where does a specified action take you from state X to Y as index changes\n",
    "actions = {\n",
    "    \"up\" : -GRID_SIZE,\n",
    "    \"down\" : GRID_SIZE,\n",
    "    \"left\" : -1,\n",
    "    \"right\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "285ec2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_lists = list(actions.keys())\n",
    "random.randint(0, len(action_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "148f54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a Value matrix\n",
    "policy = [action_lists[random.randint(0, len(action_lists)-1)] if state != GOAL_STATE else None for state in range(NUM_STATES)]\n",
    "V = np.zeros(NUM_STATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b78e5",
   "metadata": {},
   "source": [
    "# 3. Supplementary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cbd8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_validation(state: int, action: str):\n",
    "    \"\"\"\n",
    "    Function: Validates if an action can be taken by the agent in that state\n",
    "    Args:\n",
    "        state(int) : Grid position of the agent\n",
    "        action(str) :  Action it wants to take\n",
    "    Returns:\n",
    "        Bool: True for valid action and False for invalid actions\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the row, column\n",
    "    row, col = divmod(state, GRID_SIZE)\n",
    "\n",
    "    # For VERTICAL BOUNDS\n",
    "    if (row == 0 and action == \"up\") or (row == GRID_SIZE - 1 and action == \"down\"):\n",
    "        return False\n",
    "    \n",
    "    # For HORIZONTAL BOUNDS\n",
    "    if (col == 0 and action == \"left\") or (col == GRID_SIZE -1 and action == \"right\"):\n",
    "        return False\n",
    "    \n",
    "    # Else return true\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea3108e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    \"\"\"\n",
    "    Function: Validates the action and the state annd produces the next state\n",
    "    Args:\n",
    "        state (int): Position index in the grid worrld\n",
    "        action (str): The action the agent intends to take \n",
    "    Returns:\n",
    "        next_state (int) : The next state index\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the action is valid, if not return the same state back\n",
    "    if not action_validation(state=state, action=action):\n",
    "        return state\n",
    "    \n",
    "    else:\n",
    "        return state + actions[action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "364f08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid(V: np.array, GRID_SIZE: int):\n",
    "    \"\"\"\n",
    "    Function: Prints the gridfrom the value matrix and the provided GRID_SIZE\n",
    "    Args:\n",
    "        V (np.array): Value matrix with the value of each state V(s)\n",
    "        GRID_SIZE (int): The size of one of the GRID DIMENSIOONs of the square grid\n",
    "    \"\"\"\n",
    "\n",
    "    print(np.round(V.reshape((GRID_SIZE , GRID_SIZE)),2 ), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7112b8",
   "metadata": {},
   "source": [
    "# 4. Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a146af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 1 ===\n",
      "Value Function:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Policy:\n",
      "↓  ↓  →  G\n",
      "↑  ↑  ↑  ↑\n",
      "↑  ↑  ↑  ↑\n",
      "↑  ↑  ↑  ↑\n",
      "\n",
      "=== Iteration 2 ===\n",
      "Value Function:\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.   0.   0.9  1.  ]\n",
      " [0.   0.   0.81 0.9 ]\n",
      " [0.   0.   0.73 0.81]]\n",
      "Policy:\n",
      "↓  →  →  G\n",
      "↑  →  ↑  ↑\n",
      "↑  →  ↑  ↑\n",
      "↑  →  ↑  ↑\n",
      "\n",
      "=== Iteration 3 ===\n",
      "Value Function:\n",
      "[[0.   0.9  1.   0.  ]\n",
      " [0.   0.81 0.9  1.  ]\n",
      " [0.   0.73 0.81 0.9 ]\n",
      " [0.   0.66 0.73 0.81]]\n",
      "Policy:\n",
      "→  →  →  G\n",
      "→  ↑  ↑  ↑\n",
      "→  ↑  ↑  ↑\n",
      "→  ↑  ↑  ↑\n",
      "\n",
      "=== Iteration 4 ===\n",
      "Value Function:\n",
      "[[0.81 0.9  1.   0.  ]\n",
      " [0.73 0.81 0.9  1.  ]\n",
      " [0.66 0.73 0.81 0.9 ]\n",
      " [0.59 0.66 0.73 0.81]]\n",
      "Policy:\n",
      "→  →  →  G\n",
      "↑  ↑  ↑  ↑\n",
      "↑  ↑  ↑  ↑\n",
      "↑  ↑  ↑  ↑\n",
      "\n",
      "=== Iteration 5 ===\n",
      "Value Function:\n",
      "[[0.81 0.9  1.   0.  ]\n",
      " [0.73 0.81 0.9  1.  ]\n",
      " [0.66 0.73 0.81 0.9 ]\n",
      " [0.59 0.66 0.73 0.81]]\n",
      "Policy:\n",
      "→  →  →  G\n",
      "↑  ↑  ↑  ↑\n",
      "↑  ↑  ↑  ↑\n",
      "↑  ↑  ↑  ↑\n",
      "\n",
      "✅ Converged to optimal policy.\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "while True:\n",
    "    iteration += 1\n",
    "    print(f\"\\n=== Iteration {iteration} ===\")\n",
    "\n",
    "    # 1. Policy Evaluation\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for s in range(NUM_STATES):\n",
    "            if s == GOAL_STATE:\n",
    "                continue\n",
    "            a = policy[s]\n",
    "            s_next = get_next_state(s, a)\n",
    "            r = rewards[s_next]\n",
    "            new_V[s] = r + GAMMA * V[s_next]\n",
    "            delta = max(delta, abs(V[s] - new_V[s]))\n",
    "        V = new_V\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "\n",
    "    print(\"Value Function:\")\n",
    "    print(np.round(V.reshape(GRID_SIZE, GRID_SIZE), 2))\n",
    "\n",
    "    # 2. Policy Improvement\n",
    "    policy_stable = True\n",
    "    for s in range(NUM_STATES):\n",
    "        if s == GOAL_STATE:\n",
    "            continue\n",
    "\n",
    "        old_action = policy[s]\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        for a in action_lists:\n",
    "            if not action_validation(s, a):\n",
    "                continue\n",
    "            s_next = get_next_state(s, a)\n",
    "            r = rewards[s_next]\n",
    "            value = r + GAMMA * V[s_next]\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = a\n",
    "\n",
    "        policy[s] = best_action\n",
    "        if best_action != old_action:\n",
    "            policy_stable = False\n",
    "\n",
    "    # Display the current policy\n",
    "    print(\"Policy:\")\n",
    "    arrow_map = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}\n",
    "    policy_grid = []\n",
    "    for s in range(NUM_STATES):\n",
    "        if s == GOAL_STATE:\n",
    "            policy_grid.append(\"G\")\n",
    "        else:\n",
    "            policy_grid.append(arrow_map.get(policy[s], \"?\"))\n",
    "    policy_grid = np.array(policy_grid).reshape(GRID_SIZE, GRID_SIZE)\n",
    "    for row in policy_grid:\n",
    "        print(\"  \".join(row))\n",
    "\n",
    "    if policy_stable:\n",
    "        print(\"\\n✅ Converged to optimal policy.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
