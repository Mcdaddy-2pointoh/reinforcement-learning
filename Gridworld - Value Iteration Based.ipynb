{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcf6a8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9c79c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1da92",
   "metadata": {},
   "source": [
    "# 1. Environment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_conf = {\n",
    "    \"GRID_SIZE\" : 4,\n",
    "    \"GAMMA\" : 0.9,\n",
    "    \"GOAL_STATE\" : 3,\n",
    "    \"THRESHOLD\" : 0.00001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ed454",
   "metadata": {},
   "source": [
    "# 2. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83a5ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all configs into variables\n",
    "GRID_SIZE, GAMMA, GOAL_STATE, THRESHOLD = env_conf['GRID_SIZE'],  env_conf['GAMMA'], env_conf['GOAL_STATE'], env_conf['THRESHOLD']\n",
    "NUM_STATES = GRID_SIZE * GRID_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37b4371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an reward map with the specifications\n",
    "rewards = np.zeros(NUM_STATES)\n",
    "rewards[GOAL_STATE] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "466ef220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an action map, i.e. where does a specified action take you from state X to Y as index changes\n",
    "actions = {\n",
    "    \"up\" : -GRID_SIZE,\n",
    "    \"down\" : GRID_SIZE,\n",
    "    \"left\" : -1,\n",
    "    \"right\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b92a6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a Value matrix\n",
    "V = np.zeros(NUM_STATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ce52b",
   "metadata": {},
   "source": [
    "# 3. Supplementary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c8b7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_validation(state: int, action: str):\n",
    "    \"\"\"\n",
    "    Function: Validates if an action can be taken by the agent in that state\n",
    "    Args:\n",
    "        state(int) : Grid position of the agent\n",
    "        action(str) :  Action it wants to take\n",
    "    Returns:\n",
    "        Bool: True for valid action and False for invalid actions\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the row, column\n",
    "    row, col = divmod(state, GRID_SIZE)\n",
    "\n",
    "    # For VERTICAL BOUNDS\n",
    "    if (row == 0 and action == \"up\") or (row == GRID_SIZE - 1 and action == \"down\"):\n",
    "        return False\n",
    "    \n",
    "    # For HORIZONTAL BOUNDS\n",
    "    if (col == 0 and action == \"left\") or (col == GRID_SIZE -1 and action == \"right\"):\n",
    "        return False\n",
    "    \n",
    "    # Else return true\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fecaad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    \"\"\"\n",
    "    Function: Validates the action and the state annd produces the next state\n",
    "    Args:\n",
    "        state (int): Position index in the grid worrld\n",
    "        action (str): The action the agent intends to take \n",
    "    Returns:\n",
    "        next_state (int) : The next state index\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the action is valid, if not return the same state back\n",
    "    if not action_validation(state=state, action=action):\n",
    "        return state\n",
    "    \n",
    "    else:\n",
    "        return state + actions[action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8a68f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid(V: np.array, GRID_SIZE: int):\n",
    "    \"\"\"\n",
    "    Function: Prints the gridfrom the value matrix and the provided GRID_SIZE\n",
    "    Args:\n",
    "        V (np.array): Value matrix with the value of each state V(s)\n",
    "        GRID_SIZE (int): The size of one of the GRID DIMENSIOONs of the square grid\n",
    "    \"\"\"\n",
    "\n",
    "    print(np.round(V.reshape((GRID_SIZE , GRID_SIZE)),2 ), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed0d9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(V: np.array):\n",
    "    \"\"\"\n",
    "    Function: Prints the optimal policy from the Value matrix\n",
    "    Args:\n",
    "        V (np.array): The value matrix\n",
    "    \"\"\"\n",
    "    policy_arrows = []\n",
    "    for state in range(NUM_STATES):\n",
    "        if state == GOAL_STATE:\n",
    "            policy_arrows.append(\"G\")\n",
    "            continue\n",
    "\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        for action in actions:\n",
    "            if not action_validation(state, action):\n",
    "                continue\n",
    "            next_state = get_next_state(state, action)\n",
    "            r = rewards[next_state]\n",
    "            value = r + GAMMA * V[next_state]\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "\n",
    "        arrow = {\n",
    "            'up': '↑',\n",
    "            'down': '↓',\n",
    "            'left': '←',\n",
    "            'right': '→'\n",
    "        }.get(best_action, '?')\n",
    "\n",
    "        policy_arrows.append(arrow)\n",
    "\n",
    "    # Print policy grid\n",
    "    print(\"Optimal Policy:\")\n",
    "    policy_grid = np.array(policy_arrows).reshape(GRID_SIZE, GRID_SIZE)\n",
    "    for row in policy_grid:\n",
    "        print(\"  \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ede49",
   "metadata": {},
   "source": [
    "# 4. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(V: np.matrix, NUM_STATES: int, GOAL_STATE: int, GRID_SIZE: int):\n",
    "    \"\"\"\n",
    "    Function: Recurrsively iterates through the Value grid till the updates are less than THRESHOLD\n",
    "    Args:\n",
    "        V (np.matrix): The value grid\n",
    "        NUM_STATES (int): Number of states in the grid world\n",
    "        GOAL_STATE (int): The goal state index\n",
    "        GRID_SIZE (int): Dimensions of one of the sides of the grid\n",
    "    Returns: The value iterated Value Matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the iteration tracker\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Define the value confs\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        iteration += 1\n",
    "\n",
    "        \n",
    "        # Loop through and update the value of each state\n",
    "        for state in range(NUM_STATES):\n",
    "\n",
    "            # Skip updating the GOAL STATES value\n",
    "            if state == GOAL_STATE:\n",
    "                continue\n",
    "\n",
    "            # Explore each action in the state \n",
    "            # Initialise the max value as =inf \n",
    "            # Track the maximum possible value in each state\n",
    "            max_value = - np.inf\n",
    "            for action in actions.keys():\n",
    "                next_state = get_next_state(state=state, action=action)\n",
    "                r = rewards[next_state]\n",
    "                value = r + GAMMA * V[next_state]\n",
    "                max_value = max(max_value, value)\n",
    "\n",
    "            # Update the value as the max iteration \n",
    "            new_V[state] = max_value\n",
    "            delta = max(delta, abs(V[state] - new_V[state]))\n",
    "\n",
    "        # Update the value grid\n",
    "        V = new_V\n",
    "        print(f\"Iteration {iteration}\")\n",
    "\n",
    "        print_grid(V=V, GRID_SIZE=GRID_SIZE)\n",
    "\n",
    "        # Break if the delta is too low\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f33a2",
   "metadata": {},
   "source": [
    "# SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e266ea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] \n",
      "\n",
      "Iteration 2\n",
      "[[0.  0.9 1.  0. ]\n",
      " [0.  0.  0.9 1. ]\n",
      " [0.  0.  0.  0.9]\n",
      " [0.  0.  0.  0. ]] \n",
      "\n",
      "Iteration 3\n",
      "[[0.81 0.9  1.   0.  ]\n",
      " [0.   0.81 0.9  1.  ]\n",
      " [0.   0.   0.81 0.9 ]\n",
      " [0.   0.   0.   0.81]] \n",
      "\n",
      "Iteration 4\n",
      "[[0.81 0.9  1.   0.  ]\n",
      " [0.73 0.81 0.9  1.  ]\n",
      " [0.   0.73 0.81 0.9 ]\n",
      " [0.   0.   0.73 0.81]] \n",
      "\n",
      "Iteration 5\n",
      "[[0.81 0.9  1.   0.  ]\n",
      " [0.73 0.81 0.9  1.  ]\n",
      " [0.66 0.73 0.81 0.9 ]\n",
      " [0.   0.66 0.73 0.81]] \n",
      "\n",
      "Iteration 6\n",
      "[[0.81 0.9  1.   0.  ]\n",
      " [0.73 0.81 0.9  1.  ]\n",
      " [0.66 0.73 0.81 0.9 ]\n",
      " [0.59 0.66 0.73 0.81]] \n",
      "\n",
      "Iteration 7\n",
      "[[0.81 0.9  1.   0.  ]\n",
      " [0.73 0.81 0.9  1.  ]\n",
      " [0.66 0.73 0.81 0.9 ]\n",
      " [0.59 0.66 0.73 0.81]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(V=V, \n",
    "                    NUM_STATES=NUM_STATES, \n",
    "                    GOAL_STATE=GOAL_STATE, \n",
    "                    GRID_SIZE=GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4d11631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "→  →  →  G\n",
      "↑  ↑  ↑  ↑\n",
      "↑  ↑  ↑  ↑\n",
      "↑  ↑  ↑  ↑\n"
     ]
    }
   ],
   "source": [
    "extract_policy(V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
